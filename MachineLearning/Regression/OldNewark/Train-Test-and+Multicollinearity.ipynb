{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test\n",
    "\n",
    "### Model Overfit\n",
    "As I mentioned in an earlier exercise, the R^2 in regression models can always be improved by adding more predictors.  This effect occurs because you are adding information to the prediction of your dependent variable.  Note that the information may not necessarily be RELEVANT or SIGNIFICANT: it is just information.\n",
    "\n",
    "In general, the more predictors you add, the better job your model will predict that dataset.  HOWEVER, the model becomes more tied to that PARTICULAR dataset.  The model has been trained \"too well\". It may be very accurate on your sample, but it may not generalize well _beyond_ that dataset. Overfitted models explain not only the dependent variable, but also capture the noise (or error) in the data. Since noise will tend to be specific to that dataset, that capability is a bad thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Fit](model_fit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gauge model overfitting, a common practice is to separate our data into a train and a test dataset.  We then use the train dataset to develop the model, and then use the test dataset to gauge its generalizability.\n",
    "\n",
    "I should also mention model _underfitting_, which is generally less of a concern: it simply means your model does not fit the training set well. In the above example, the model does not capture the nonlinear trend in the data, so its fit wil be poor, especially for lower and higher values.\n",
    "\n",
    "__One important note before we continue__: I am using the Newark real estate dataset again, mainly because you know it well.  The rule of thumb is about 30-50 observations per predictor (or X variable).  That means a model with four predictors needs 120-200 rows of data.\n",
    "\n",
    "But there is another problem: if we divide the dataset into 70% for training, that means we need 40-70 rows to detect model overfit.  That means 160-280 rows! \n",
    "\n",
    "With 166 rows in the Newark dataset (less if we drop the missing value rows), we barely have enough data for 4 predictors/features.  We should therefore be careful of model overfit, which we explore next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('OldNewark.csv')\n",
    "temp = df.loc[df['SQFT'] < 4000].copy() # remove outliers\n",
    "df2 = temp.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'STREETNO', 'STREET', 'ZESTIMATE', 'SQFT', 'BEDR', 'BATHR',\n",
       "       'YRBUILT', 'LOTSIZE', 'SOLDFOR', 'YRSOLD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 11)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "Since we have done a lot of swimming in the Newark dataset, I am skipping it for now.  As you have learned, however, swimming is an important part of any analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['FACEWEST'] = df2['STREETNO'] % 2\n",
    "df2['ORCHARD'] = 0\n",
    "df2.loc[df2['STREET'] == 'Orchard', 'ORCHARD'] = 1\n",
    "df2['AGE'] = 2019 - df2['YRBUILT']\n",
    "\n",
    "lotsqft = df2['LOTSIZE'] * 43560 # convert pct to sqft\n",
    "df2['COVERAGE'] = (df2['SQFT']/2) / lotsqft # assume two stories. Half of house is on the ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X and Y dataframes\n",
    "X = df2[['SQFT', 'FACEWEST', 'AGE','COVERAGE']].copy()\n",
    "Y = df2['ZESTIMATE'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into Train and Test\n",
    "You will see the following code for the rest of the semester, since testing for model overfit is a fundamental requirement when evaluating any ML model.  The following code splits the data into four dataframes--X and Y for training and X amd Y for testing.\n",
    "\n",
    "Fortunately, sklearn provides an easy function that randomly selects rows from our dataframe.  The random_state argument allows you to replicate the results--that is, setting the number to 2 ensures you will get the same results as me when you run the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into the Training set and Test set\n",
    "# test_size reflects the % to use in the test set.  Usually set to around 0.2, but we\n",
    "# have a small sample\n",
    "# To get the same results every time, set random_state to the same value (e.g., 0).\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, \\\n",
    "        test_size = 0.5, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              ZESTIMATE   R-squared:                       0.471\n",
      "Model:                            OLS   Adj. R-squared:                  0.432\n",
      "Method:                 Least Squares   F-statistic:                     12.02\n",
      "Date:                Wed, 11 Nov 2020   Prob (F-statistic):           4.70e-07\n",
      "Time:                        14:19:38   Log-Likelihood:                -706.03\n",
      "No. Observations:                  59   AIC:                             1422.\n",
      "Df Residuals:                      54   BIC:                             1432.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       7.666e+04   3.82e+04      2.005      0.050      11.256    1.53e+05\n",
      "SQFT          47.9495     14.480      3.311      0.002      18.919      76.980\n",
      "FACEWEST    6847.5018   1.08e+04      0.635      0.528   -1.48e+04    2.85e+04\n",
      "AGE         1500.8489    493.793      3.039      0.004     510.853    2490.844\n",
      "COVERAGE    2.892e+05   3.19e+05      0.907      0.368    -3.5e+05    9.28e+05\n",
      "==============================================================================\n",
      "Omnibus:                        3.654   Durbin-Watson:                   2.542\n",
      "Prob(Omnibus):                  0.161   Jarque-Bera (JB):                2.795\n",
      "Skew:                           0.359   Prob(JB):                        0.247\n",
      "Kurtosis:                       3.789   Cond. No.                     1.35e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.35e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "Average error: 39814.97.\n"
     ]
    }
   ],
   "source": [
    "# Run regression using statsmodels\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train = sm.add_constant(X_train) # required if constant expected\n",
    "est = sm.OLS(y_train,X_train).fit() # fit model\n",
    "predictions = est.predict() # get predicted values\n",
    "print(est.summary()) # prints full regression results\n",
    "print(\"\\nAverage error: {:.2f}.\".format(math.sqrt(est.mse_resid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "The process for testing the model is identical to training the model.  We simply compare the test results to the train results.  The following parameters should be comparable:\n",
    "* R^2 and adj. R^2, especially the difference between the two.  \n",
    "* Regression coefficients (especially be concerned if the sign flips or significant predictors become insignificant...or vice versa)\n",
    "* Error levels, including the final model error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              ZESTIMATE   R-squared:                       0.402\n",
      "Model:                            OLS   Adj. R-squared:                  0.357\n",
      "Method:                 Least Squares   F-statistic:                     9.060\n",
      "Date:                Wed, 11 Nov 2020   Prob (F-statistic):           1.13e-05\n",
      "Time:                        14:19:39   Log-Likelihood:                -720.38\n",
      "No. Observations:                  59   AIC:                             1451.\n",
      "Df Residuals:                      54   BIC:                             1461.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       9.923e+04   4.18e+04      2.372      0.021    1.54e+04    1.83e+05\n",
      "SQFT          86.1676     18.085      4.765      0.000      49.910     122.425\n",
      "FACEWEST    2514.2024   1.35e+04      0.186      0.853   -2.46e+04    2.96e+04\n",
      "AGE         1016.2440    432.558      2.349      0.022     149.018    1883.470\n",
      "COVERAGE   -5.506e+05   4.01e+05     -1.373      0.175   -1.35e+06    2.53e+05\n",
      "==============================================================================\n",
      "Omnibus:                       58.488   Durbin-Watson:                   1.902\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              319.749\n",
      "Skew:                           2.809   Prob(JB):                     3.69e-70\n",
      "Kurtosis:                      12.925   Cond. No.                     1.30e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.3e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "Average error: 50771.21.\n"
     ]
    }
   ],
   "source": [
    "# Run regression using statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_test = sm.add_constant(X_test) # required if constant expected\n",
    "est = sm.OLS(y_test,X_test).fit() # fit model\n",
    "predictions = est.predict() # get predicted values\n",
    "print(est.summary()) # prints full regression results\n",
    "print(\"\\nAverage error: {:.2f}.\".format(math.sqrt(est.mse_resid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems and more Problems\n",
    "If you compare the train results with the test results, you will see big differences in the model's explanation and prediction metrics.  So what is going on?\n",
    "\n",
    "Recall that I wrote that we were pushing the model a bit, given our limited sample size.  The model overfit may be a result of us pushing this model beyond where we should.  With so little data, overfitting and coefficient instability isnt a surprise.  \n",
    "\n",
    "Notice the warnings under the model.  Take a look especially at warning 2.  This indicates that there may also be another issue--multicollinearity--which I will visit next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "In general, you want your target column to be correlated with your features (correlation is after all the basis of regression). Multicollinearity, however, refers to high correlations among a model's *features*.  But correlation among features reflects duplicate (or redundant) information in the model.  When the same information is entered multiple times, the end result is unpredictable.  Multicollinearity does not affect R^2 values, but can greatly distort the values of your coefficients.\n",
    "\n",
    "If you look at the above model, there is a warning which may indicate multicollinearity (#2 at the bottom).  To diagnose the problem, lets run the VIF (variance inflation factor) scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF:    VIF Factor  features\n",
      "0   40.043273     const\n",
      "1    1.901000      SQFT\n",
      "2    1.041476  FACEWEST\n",
      "3    1.006898       AGE\n",
      "4    1.860831  COVERAGE\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "variables = est.model.exog # get model features\n",
    "vif = pd.DataFrame() # create a dataframe\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n",
    "vif[\"features\"] = X_test.columns\n",
    "print('VIF: {}'.format(vif))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, features with VIF values > 2 should be examined.  Note that SQFT is high, but so is COVERAGE.  Does it make sense that COVERAGE and SQFT are correlated?  Let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\style.py:1089: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(s.to_numpy()) if vmin is None else vmin\n",
      "C:\\Users\\Mark\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\style.py:1090: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(s.to_numpy()) if vmax is None else vmax\n",
      "C:\\Users\\Mark\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py:527: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_da81a182_2452_11eb_9196_305a3a5a193frow0_col0 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow0_col1 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow0_col2 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow0_col3 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow0_col4 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow1_col0 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow1_col1 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow1_col2 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow1_col3 {\n",
       "            background-color:  #eee9f3;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow1_col4 {\n",
       "            background-color:  #1278b4;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow2_col0 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow2_col1 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow2_col2 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow2_col3 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow2_col4 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow3_col0 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow3_col1 {\n",
       "            background-color:  #dddbec;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow3_col2 {\n",
       "            background-color:  #f1ebf5;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow3_col3 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow3_col4 {\n",
       "            background-color:  #ece7f2;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow4_col0 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow4_col1 {\n",
       "            background-color:  #0c74b2;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow4_col2 {\n",
       "            background-color:  #f7f0f7;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow4_col3 {\n",
       "            background-color:  #f2ecf5;\n",
       "            color:  #000000;\n",
       "        }    #T_da81a182_2452_11eb_9196_305a3a5a193frow4_col4 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_da81a182_2452_11eb_9196_305a3a5a193f\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >const</th>        <th class=\"col_heading level0 col1\" >SQFT</th>        <th class=\"col_heading level0 col2\" >FACEWEST</th>        <th class=\"col_heading level0 col3\" >AGE</th>        <th class=\"col_heading level0 col4\" >COVERAGE</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_da81a182_2452_11eb_9196_305a3a5a193flevel0_row0\" class=\"row_heading level0 row0\" >const</th>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow0_col0\" class=\"data row0 col0\" >nan</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow0_col1\" class=\"data row0 col1\" >nan</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow0_col3\" class=\"data row0 col3\" >nan</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_da81a182_2452_11eb_9196_305a3a5a193flevel0_row1\" class=\"row_heading level0 row1\" >SQFT</th>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow1_col0\" class=\"data row1 col0\" >nan</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow1_col2\" class=\"data row1 col2\" >-0.187469</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow1_col3\" class=\"data row1 col3\" >0.043300</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow1_col4\" class=\"data row1 col4\" >0.680068</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_da81a182_2452_11eb_9196_305a3a5a193flevel0_row2\" class=\"row_heading level0 row2\" >FACEWEST</th>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow2_col0\" class=\"data row2 col0\" >nan</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow2_col1\" class=\"data row2 col1\" >-0.187469</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow2_col3\" class=\"data row2 col3\" >-0.076238</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow2_col4\" class=\"data row2 col4\" >-0.122782</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_da81a182_2452_11eb_9196_305a3a5a193flevel0_row3\" class=\"row_heading level0 row3\" >AGE</th>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow3_col0\" class=\"data row3 col0\" >nan</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow3_col1\" class=\"data row3 col1\" >0.043300</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow3_col2\" class=\"data row3 col2\" >-0.076238</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow3_col4\" class=\"data row3 col4\" >0.019655</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_da81a182_2452_11eb_9196_305a3a5a193flevel0_row4\" class=\"row_heading level0 row4\" >COVERAGE</th>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow4_col0\" class=\"data row4 col0\" >nan</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow4_col1\" class=\"data row4 col1\" >0.680068</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow4_col2\" class=\"data row4 col2\" >-0.122782</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow4_col3\" class=\"data row4 col3\" >0.019655</td>\n",
       "                        <td id=\"T_da81a182_2452_11eb_9196_305a3a5a193frow4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2176acb3188>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = X_test.corr()\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ignore the nan entries: they occur because the const column is all ones.\n",
    "\n",
    "The correlation between COVERAGE and SQFT is moderately high.  In hindsight, it makes sense that SQFT and COVERAGE would be correlated, since larger houses have greater coverage (the lot sizes in Old Newark are fairly similar)! Please note that it is appropriate to explore them both as predictors! Whether or not they offer unique information needs to be tested.  In general, however, features that have a correlation > 0.50 need to be examined carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove COVERAGE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2[['SQFT', 'FACEWEST', 'AGE']].copy() # remove COVERAGE\n",
    "Y = df2['ZESTIMATE'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, \\\n",
    "        test_size = 0.5, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              ZESTIMATE   R-squared:                       0.463\n",
      "Model:                            OLS   Adj. R-squared:                  0.434\n",
      "Method:                 Least Squares   F-statistic:                     15.80\n",
      "Date:                Wed, 11 Nov 2020   Prob (F-statistic):           1.57e-07\n",
      "Time:                        14:20:42   Log-Likelihood:                -706.48\n",
      "No. Observations:                  59   AIC:                             1421.\n",
      "Df Residuals:                      55   BIC:                             1429.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       7.026e+04   3.75e+04      1.873      0.066   -4916.221    1.45e+05\n",
      "SQFT          57.8264      9.531      6.067      0.000      38.727      76.926\n",
      "FACEWEST    9125.4795   1.05e+04      0.871      0.387   -1.19e+04    3.01e+04\n",
      "AGE         1616.4158    476.308      3.394      0.001     661.872    2570.959\n",
      "==============================================================================\n",
      "Omnibus:                        3.462   Durbin-Watson:                   2.503\n",
      "Prob(Omnibus):                  0.177   Jarque-Bera (JB):                2.561\n",
      "Skew:                           0.363   Prob(JB):                        0.278\n",
      "Kurtosis:                       3.717   Cond. No.                     1.59e+04\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.59e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "Average error: 39750.83.\n"
     ]
    }
   ],
   "source": [
    "X_train = sm.add_constant(X_train) # required if constant expected\n",
    "est = sm.OLS(y_train,X_train).fit() # fit model\n",
    "predictions = est.predict() # get predicted values\n",
    "print(est.summary()) # prints full regression results\n",
    "print(\"\\nAverage error: {:.2f}.\".format(math.sqrt(est.mse_resid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              ZESTIMATE   R-squared:                       0.381\n",
      "Model:                            OLS   Adj. R-squared:                  0.347\n",
      "Method:                 Least Squares   F-statistic:                     11.27\n",
      "Date:                Wed, 11 Nov 2020   Prob (F-statistic):           7.20e-06\n",
      "Time:                        14:21:14   Log-Likelihood:                -721.39\n",
      "No. Observations:                  59   AIC:                             1451.\n",
      "Df Residuals:                      55   BIC:                             1459.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       9.363e+04    4.2e+04      2.231      0.030    9533.807    1.78e+05\n",
      "SQFT          69.4269     13.466      5.156      0.000      42.440      96.414\n",
      "FACEWEST    2409.8201   1.36e+04      0.177      0.860   -2.49e+04    2.97e+04\n",
      "AGE         1023.9348    435.992      2.349      0.022     150.188    1897.681\n",
      "==============================================================================\n",
      "Omnibus:                       59.007   Durbin-Watson:                   1.996\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              331.377\n",
      "Skew:                           2.826   Prob(JB):                     1.10e-72\n",
      "Kurtosis:                      13.142   Cond. No.                     1.36e+04\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.36e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "Average error: 51178.56.\n"
     ]
    }
   ],
   "source": [
    "X_test = sm.add_constant(X_test) # required if constant expected\n",
    "est = sm.OLS(y_test,X_test).fit() # fit model\n",
    "predictions = est.predict() # get predicted values\n",
    "print(est.summary()) # prints full regression results\n",
    "print(\"\\nAverage error: {:.2f}.\".format(math.sqrt(est.mse_resid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF:    VIF Factor  features\n",
      "0   39.661607     const\n",
      "1    1.037335      SQFT\n",
      "2    1.041443  FACEWEST\n",
      "3    1.006729       AGE\n"
     ]
    }
   ],
   "source": [
    "variables = est.model.exog # get model features\n",
    "vif = pd.DataFrame() # create a dataframe\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n",
    "vif[\"features\"] = X_train.columns\n",
    "print('VIF: {}'.format(vif))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, multicollinearity is no longer a problem.  I leave it to you to run the test dataset.  \n",
    "\n",
    "The error at the end of the regression output remains much higher for the test than for the train.  Remember, that the concerns with sample size remain, and that is the likely cause of the warning that is still generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the bottom line?\n",
    "The purpose of separating a data set into train and test sets is to see whether the models for the two different samples are comparable.  If the training model's fit is substantively better than the test model, your regression model may be too complicated for the data.  This can happen when you have too many X's (or features) in your model.  Smaller sample sizes (especially ones less than 100) are susceptible to overfitting.  \n",
    "\n",
    "How do you tell if your model is overfitted (or has other diagnostic problems):\n",
    "* R2 and error levels differ in the train and test set.\n",
    "* The regression coefficients differ, especially in the signs and/or the significance levels \n",
    "* If you have sufficient sample size, be aware of coefficients that are highly significant in one model, but less so in the other.\n",
    "* The general rule of thumb is that you need 30 observations for each X variable in your model.  If you have three X's, that would mean you need about 100 observations.  BUT remember that you are splitting into a train and a test set! That means you need 200 observations!\n",
    "\n",
    "### Summary of important points:\n",
    "* In practice, the chances for overfitting a linear regression model is probably small, *if you have sufficient sample size*.  \n",
    "* Always review the coefficient values and signs to ensure they make sense.  \n",
    "* If you have a negative coefficient that should be positive (or vice versa), you may have two (or more) X variables that are highly correlated.  That will result in model instability and wacky values for your regression coefficients.  \n",
    "* Always run a correlation matrix for your _X_ variable to confirm their correlations are less than 0.5. If some are higher than that, be sure to run the VIF stats to confirm the VIFs are less than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
